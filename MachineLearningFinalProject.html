<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=m0tazYRimFnV1hoGKbgtnw');.lst-kix_w8q038tzqb2z-4>li:before{content:"\0025cb  "}.lst-kix_9n98b820frfe-2>li:before{content:"\0025a0  "}.lst-kix_9n98b820frfe-3>li:before{content:"\0025cf  "}.lst-kix_w8q038tzqb2z-3>li:before{content:"\0025cf  "}.lst-kix_w8q038tzqb2z-5>li:before{content:"\0025a0  "}.lst-kix_w8q038tzqb2z-2>li:before{content:"\0025a0  "}.lst-kix_w8q038tzqb2z-6>li:before{content:"\0025cf  "}.lst-kix_9n98b820frfe-1>li:before{content:"\0025cb  "}.lst-kix_9n98b820frfe-5>li:before{content:"\0025a0  "}.lst-kix_9n98b820frfe-4>li:before{content:"\0025cb  "}ul.lst-kix_w8q038tzqb2z-6{list-style-type:none}ul.lst-kix_w8q038tzqb2z-5{list-style-type:none}ul.lst-kix_w8q038tzqb2z-8{list-style-type:none}ul.lst-kix_w8q038tzqb2z-7{list-style-type:none}.lst-kix_w8q038tzqb2z-0>li:before{content:"\0025cf  "}.lst-kix_9n98b820frfe-6>li:before{content:"\0025cf  "}.lst-kix_9n98b820frfe-7>li:before{content:"\0025cb  "}ul.lst-kix_w8q038tzqb2z-0{list-style-type:none}.lst-kix_w8q038tzqb2z-1>li:before{content:"\0025cb  "}ul.lst-kix_w8q038tzqb2z-2{list-style-type:none}ul.lst-kix_w8q038tzqb2z-1{list-style-type:none}.lst-kix_9n98b820frfe-8>li:before{content:"\0025a0  "}ul.lst-kix_w8q038tzqb2z-4{list-style-type:none}ul.lst-kix_w8q038tzqb2z-3{list-style-type:none}ul.lst-kix_9n98b820frfe-2{list-style-type:none}ul.lst-kix_9n98b820frfe-3{list-style-type:none}ul.lst-kix_9n98b820frfe-4{list-style-type:none}ul.lst-kix_9n98b820frfe-5{list-style-type:none}ul.lst-kix_9n98b820frfe-0{list-style-type:none}ul.lst-kix_9n98b820frfe-1{list-style-type:none}.lst-kix_w8q038tzqb2z-8>li:before{content:"\0025a0  "}.lst-kix_w8q038tzqb2z-7>li:before{content:"\0025cb  "}ul.lst-kix_9n98b820frfe-6{list-style-type:none}.lst-kix_9n98b820frfe-0>li:before{content:"\0025cf  "}ul.lst-kix_9n98b820frfe-7{list-style-type:none}ul.lst-kix_9n98b820frfe-8{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c6{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c4{background-color:#ffffff;color:#222222;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c12{background-color:#ffffff;color:#222222;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c15{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c2{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c9{padding-top:12pt;text-indent:36pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c0{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c21{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c13{padding-top:12pt;padding-bottom:12pt;line-height:0.5;orphans:2;widows:2;text-align:center}.c17{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c3{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c5{background-color:#ffffff;font-size:12pt;font-family:"Times New Roman";color:#222222;font-weight:400}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c22{text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c18{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c27{border-spacing:0;border-collapse:collapse;margin-right:auto}.c14{background-color:#ffffff;font-family:"Times New Roman";color:#222222;font-weight:700}.c10{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c29{color:#000000;font-weight:400;font-family:"Arial"}.c26{font-weight:400;font-size:20pt;font-family:"Times New Roman"}.c20{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c19{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c25{font-weight:400;font-family:"Cambria"}.c11{height:11pt}.c23{font-style:italic}.c1{height:0pt}.c28{height:32.2pt}.c24{font-size:12pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c19"><div><p class="c7 c11"><span class="c22 c29"></span></p></div><p class="c7 c11"><span class="c18 c24 c25"></span></p><p class="c7 c11"><span class="c18 c25 c24"></span></p><h1 class="c15" id="h.dzbe48ikj74h"><span class="c18 c26">Detecting COVID-19 from Chest X-Ray Images</span></h1><hr><p class="c7 c11"><span class="c0"></span></p><p class="c7 c11"><span class="c0"></span></p><p class="c21 c11"><span class="c18 c10"></span></p><p class="c21"><span class="c16">Abdullah AlOthman</span></p><p class="c21"><span class="c16">&nbsp;Felipe Buchbinder</span></p><p class="c21"><span class="c16">Joseph Krinke</span></p><p class="c21"><span class="c16">JingJing Shi</span></p><p class="c21 c11"><span class="c16"></span></p><p class="c21"><span class="c16">Duke MIDS</span></p><p class="c3"><span class="c16">April 2020</span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c17"><span class="c20">Abstract</span></p><p class="c9"><span class="c10">Millions of people have been infected with the novel coronavirus COVID-19. The current technology used to detect COVID-19 is a PCR-based test, which many countries have had difficulty producing, administering, and analyzing. Examining lung images has been proposed as a method to identify patients suffering from the disease, but it is time-consuming for radiologists to analyze the images manually. Artificial intelligence has been proposed as a way to efficiently identify COVID-19 cases using lung images</span><span class="c10">. In this paper we create a novel dataset and use it to fine-tune an InceptionV3 and an </span><span class="c10">Xception</span><span class="c10">&nbsp;model and use them in an ensemble. Our</span><span class="c10">&nbsp;</span><span class="c10">results show our model can distinguish between disease classes, achieving </span><span class="c10">89% overall accuracy with 97% precision and 93% recall</span><span class="c10">&nbsp;on the COVID-19 class</span><span class="c10">.</span><span class="c10">&nbsp;</span><span class="c10">This report, however, is merely a feasibility study, as existing datasets are </span><span class="c10">not yet reliable enough to be used in earnest</span><span class="c10">.</span></p><p class="c17"><span class="c0">Introduction &nbsp;</span></p><p class="c9"><span class="c18 c10">Viruses are infectious agents that can only replicate within the cells of living hosts [1]. Disease outbreaks caused by viruses often pose a risk to public health [2]. In the past two decades, lower respiratory tract infections caused by viruses have become the most deadly infectious diseases [3]. There have been several outbreaks caused by influenza and coronaviruses. These include severe acute respiratory syndrome (SARS) in 2003 [4] and Middle East respiratory syndrome (MERS) in 2012 [5]. </span></p><p class="c9"><span class="c10 c18">Like SARS and MERS, the COVID-19 is a highly contagious pathogen, and has spread to 210 countries with a total of 1,852,356 confirmed cases in four months as of April 12th, 2020[6]. Lessons from past diseases have catalyzed the sharing of genetic information of COVID-19 globally, and this information has enabled the rapid development of diagnostic tests. The COVID-19 reverse-transcription-polymerase-chain-reaction (RT-PCR) test is the current diagnostic test for COVID-19, and it is used for the in-vitro qualitative detection of COVID-19 RNA in upper and lower respiratory specimens taken from individuals [7]. However, there are several limitations to the COVID-19 RT-PCR test, which lead to difficulties when attempting to implement mass testing. The test produces large numbers of false negatives such that the CDC recommends that healthcare providers combine results with symptom, exposure, and geographic information to make a diagnosis[8]. Additionally, many countries do not have the necessary laboratory space allocated to process tests; only about 140,000 tests have been done daily in the US, which is not sufficient given the speed of the spread of COVID-19. Consequently, a different diagnostic method may prove useful in helping governments track and fight the disease.</span></p><p class="c9"><span class="c18 c10">This has motivated the scientific community to look for alternative ways to detect the virus. Computer Tomography (CT) scans and X-rays of the patient&rsquo;s lungs (collectively called chest radiographs) are established methods of diagnosing the presence of COVID-19, with some even arguing that a chest CT is indispensable for the proper diagnosis of this disease[9]. Indeed, Li &amp; Xia[10] found that doctors failed to correctly identify COVID-19 in CT images in only 3.9% of cases. A large study in China[11] showed that chest CT-scans have a high recall for COVID-19 diagnosis, and might be considered for the COVID-19 screening, especially in high epidemical areas. Since CT and X-ray machines are much more widespread than COVID-19 tests, detecting COVID-19 from these images could be an important weapon in confronting this disease.</span></p><p class="c9"><span class="c10">CT-scans and X-rays are diagnostic tests that are already </span><span class="c10">available throughout the world. In addition, most medical personnel are already trained in operating X-ray machines, so no additional training is necessary. Moreover, X-ray diagnostics results are ready quickly, reducing the time lapse between examination and intervention. </span><span class="c10">This</span><span class="c10">&nbsp;speed could be further increased if a</span><span class="c10">&nbsp;</span><span class="c10">faster method of examining chest radiographs was developed.</span><span class="c10">&nbsp;</span><span class="c10">This paper aims to use machine-learning techniques to quickly identify the presence of COVID-19 in a chest radiograph, thereby demonstrating a potential automatic diagnostic technique.</span></p><p class="c17"><span class="c0">Background</span></p><p class="c9"><span class="c18 c10">One of the major challenges the world is facing with the COVID-19 pandemic is the shortage of testing. Many countries, including the US, face a shortage of tests, with demands for testing greatly exceeding the available supply[12, 13]. As a result, a number of scholars have attempted to develop machine learning approaches to detect COVID-19 from CT or X-ray images.This follows developments applied in the contexts of other diseases, such as SARS[14] and pneumonia[15, 16]. Narin and others[17] used a ResNet50 with transfer learning and obtained 98% accuracy on a dataset of 50 X-rays of COVID-19 patients and 50 X-rays of healthy patients. Not only is this a very small sample, it is also perfectly balanced, which is unrealistic in practice. &nbsp;A similar result is attained by Salman[18], who correctly classified all 260 images of a balanced sample of COVID-19 and healthy patients. Yet, this study is subject to the same caveats as Narin[17].</span></p><p class="c9"><span class="c4">Zhang et al.[19] use a much larger dataset, with 100 COVID-19 patients and 1431 healthy patients. They propose a convolutional neural network (CNN) built on top of a pre-trained ImageNet backbone. With it, the authors managed to accurately identify 96% of diseased patients, but only 76% of the healthy ones. Feng et al.[20] tackled the somewhat different problem of distinguishing between images of people who have COVID-19 and those of people who have viral pneumonia. This is arguably a more difficult problem than separating between diseased and healthy patients. Still, they achieve a recall of 90.70% and a specificity of 83.30% on a dataset with 1658 COVID-19 patients and 1028 viral pneumonia patients. Unlike Narin[17] or Zhang et al.[19], however, Feng et al. base their analysis in CT, rather than X-ray imaging.</span></p><p class="c9"><span class="c5">To put these model&rsquo;s performances into perspective, we may recall </span><span class="c10">Li and Xia`s (2020)</span><span class="c5">&nbsp;finding that doctors failed to identify COVID-19 in only 3.9% of CT images, meaning doctors had a recall of 96.1%. This is much higher than Feng </span><span class="c5 c23">et al.</span><span class="c5">&rsquo;s [20] 90.7%, which has also used CT images, but is at par with Zhang </span><span class="c5 c23">et al</span><span class="c5">`s [19] model constructed with X-ray imaging. The benchmark of medical error rate is more clearly outperformed by Apostolopoulos and Mpesiana[21], who use </span><span class="c5">transfer</span><span class="c5">&nbsp;learning on a series of state-of-the-art CNNs. The authors use a dataset with 224 images with confirmed COVID-19, 700 images with confirmed common bacterial pneumonia, and 504 images of healthy patients. They find that a VGG-19 network achieves a recall of 92.85% and a specificity of 98.75%. &nbsp;The authors argue, however, that recall is more important than specificity when dealing with health issues, and therefore preferred the MobileNet v2 architecture, which achieved a recall of 99.10% and a specificity of 97.09%. </span><span class="c5">Note that Apostolopoulos and Mpesiana [21] do not distinguish between viral and bacterial pneumonia, a critique they acknowledge themselves. They therefore proceed to test the MobileNet v2 architecture on a new dataset which allows for such distinction.</span><span class="c5">&nbsp;But only the MobileNet v2 architecture is run on this new dataset, and it produced a recall of 98.66% and a specificity of 96.46%.</span></p><p class="c9"><span class="c5">These analyses show the potential </span><span class="c5">of applying machine learning techniques to the problem of detecting COVID-19 using chest radiographs. H</span><span class="c5">owever, they are limited by their small sample sizes</span><span class="c4">. In this paper we address these limitations by combining several lung-disease datasets to produce an image collection containing images of normal lungs, those with pneumonia, and COVID-19. </span></p><p class="c17"><span class="c14 c24">Data</span></p><p class="c17"><span class="c4">COVID-19 Radiography Database [22][23]</span></p><p class="c9"><span class="c5">The COVID-19 dataset was collected by a research team from Qatar University, Doha, Qatar and the University of Dhaka, Bangladesh with their collaborators from Pakistan and Malaysia. The dataset contains 219 COVID-19 positive cases, 1,341 normal cases and 1,345 viral pneumonia cases. </span><span class="c4">All the images are in PNG file format and their dimensions are 1024-by-1024 pixels.</span></p><p class="c17"><span class="c5">NIH Chest X-ray Data [24]</span></p><p class="c9"><span class="c5">The NIH X-ray dataset consists of 112,120 labeled X-ray images from 30805 unique patients. The labels were constructed using natural language processing but are expected to be over 90% accurate. The images all have dimensions of 1024x1024. We used </span><span class="c5">644 images from this dataset</span><span class="c5">; 322 of adult normal lungs and </span><span class="c5">322 of adult lungs with pneumonia</span><span class="c4">.We used this number as it was all the pneumonia images available in the dataset.</span></p><p class="c3"><span class="c14 c24">Table 1: Distribution of Classes Across Datasets</span></p><a id="t.7c53eb2af0d3394d4346c848749907a3bc52f828"></a><a id="t.0"></a><table class="c27"><tbody><tr class="c1"><td class="c6" colspan="1" rowspan="1"><p class="c8 c11"><span class="c4"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c12">COVID-19 Database</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c12">NIH</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c12">Total</span></p></td></tr><tr class="c28"><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c12">Normal</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">1341</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">322</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">2069</span></p></td></tr><tr class="c1"><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c12">Pneumonia</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">1345</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">322</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">1667</span></p></td></tr><tr class="c1"><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c12">COVID-19</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">219</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c4">0</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c5">219</span></p></td></tr></tbody></table><p class="c3 c11"><span class="c4"></span></p><p class="c9"><span class="c5">In the COVID-19 dataset, 21 duplicate images were removed from the analysis. This is a developing situation, so the images available for analysis are limited and have imperfections</span><span class="c5">. It is also important to note that the files from the COVID-19 database had a mixture of adult and child images, with all of the pneumonia pictures being of children. </span><span class="c5">To address this w</span><span class="c5">e added additional data from </span><span class="c5">the NIH Chest X-ray Dataset [24] </span><span class="c5">containing adult pneumonia images to ensure that the model was not inadvertently learning to identify X-rays of children. </span></p><p class="c17"><span class="c12">Methods</span></p><p class="c9"><span class="c5">We split the dataset into training, validation, and test subsets with sizes of 70%, 10%, and 20%, respectively. </span><span class="c5">The split is stratified based on the class distribution, meaning that each subset has the same distribution of classes.</span><span class="c5">&nbsp;We augment the data with random horizontal flipping and rotations, we then scale the images to 299x299 to feed into the model. We used an </span><span class="c5">InceptionV3 model and an Xception model</span><span class="c5">&nbsp;(see Figure 1) both pre-trained on the ImageNet dataset and replaced the classification layers with our own. </span><span class="c5">Since the dataset is imbalanced, </span><span class="c4">we assigned different weights to the 3 classes based on their relative proportions in the training set. After exploring the search space by monitoring the performance on the validation set, we decided to use the Adam optimizer with a learning rate of 1e-4 and a decay of 1e-6. We use categorical cross-entropy for our loss function. We then weigh the outputs of the models based on their performance on the validation set and then average those weighted outputs to determine the class of the image. Our methodology is summarized in Figure 2. </span></p><p class="c3 c11"><span class="c4"></span></p><p class="c17"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 578.67px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 578.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c14 c24">Figure 1. </span><span class="c12">Schematic diagrams of the InceptionV3 and the Xception models </span></p><p class="c13"><span class="c14 c24">(compressed view) [25].</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 340.01px; height: 663.50px;"><img alt="" src="images/image2.png" style="width: 968.79px; height: 663.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c14 c24">Figure 2. </span><span class="c20">Diagram of Data Collection and Modeling Process </span></p><p class="c3 c11"><span class="c4"></span></p><p class="c17"><span class="c12">Results</span></p><p class="c9"><span class="c5">The test set is composed of 706 images: 40 COVID-19 cases, 333 normal cases and 333 other/pneumonia cases. Figure 3 displays the performance achieved on this test set for the Inception V3 and for the Xception model, as well as for an </span><span class="c5 c23">ensemble </span><span class="c5">model that computes the weighted average of the votes of these two models. The InceptionV3 has better recall, whereas the Xception has better </span><span class="c5">precision</span><span class="c5">. The weighted model achieves the best result, with an</span><span class="c5">&nbsp;</span><span class="c5">89% overall accuracy score, a 93% recall</span><span class="c5">&nbsp;and a </span><span class="c5">97% precision </span><span class="c5">for the COVID-19 class. Figure 3 displays the confusion matrix and the ROC curves for each category. </span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 280.00px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 280.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 268.00px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 268.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c14 c24">Figure 3. Model Performance on Test Set</span></p><p class="c13"><span class="c14 c22">[other indicates pneumonia]</span></p><p class="c9"><span class="c5">Examining misclassified images may give some insight into where and why our model fails, but we must acknowledge the limited number of samples and our lack of medical expertise when making any hypothesis. </span><span class="c5">It may be that the variable sources of the images had a confounding effect on the classifications. </span><span class="c4">For example, the COVID-19 images are collected from multiple sources and publications which means that they have higher &lsquo;noise&rsquo; signals than the other classes (See Figure 4). </span></p><p class="c9"><span class="c5">It is also important to consider the limitations of the images themselves. It is possible that by the time that a COVID-19-positive individual receives a chest scan their disease has progressed significantly. I</span><span class="c5">t may be that our model would not be effective in early detection of the disease</span><span class="c5">, as those patients may not have sufficient lung damage to be detected. &nbsp;Additionally, the datasets used have not been peer-reviewed or collected in a standardized fashion. Images were taken on people of varying ages and sizes, on different machines, and those images were processed differently. There are also images marked by physicians which</span><span class="c5">&nbsp;could lead to possible information leakage</span><span class="c4">.</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 470.67px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 470.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c12">Figure 4. Samples of Classified Images </span></p><p class="c3"><span class="c14">[</span><span class="c14">other indicates pneumonia, b</span><span class="c14">lanks indicate that images with that actual and predicted label combination do not exist</span><span class="c22 c14">&nbsp;in the test set]</span></p><p class="c3 c11"><span class="c12"></span></p><p class="c9"><span class="c4">We are particularly interested in distinguishing COVID-19 cases from non-COVID-19 cases. When performing this distinction, our model achieves an AUC of 0.99. &nbsp;While this distinction is the most important, our model also achieves a 0.98 AUC in identifying both healthy patients (&ldquo;Normal&rdquo;) &nbsp;and patients with other forms of pneumonia (&ldquo;Other&rdquo;).</span></p><p class="c9"><span class="c5">In the literature, the model which is most closely comparable to ours is that of </span><span class="c5">Apostolopoulos and Mpesiana [21]</span><span class="c5">, since they also classify patients as having COVID-19, being normal, or having </span><span class="c5">pneumonia</span><span class="c5">. Their model is also the best so far, so it serves as a competitive benchmark. Their model achieves a recall of </span><span class="c5">98.66% and a specificity of 96.46%</span><span class="c5">. Our model has a recall of 93% and a specificity of 99.8%. </span><span class="c5">Apostolopoulos and Mpesiana&rsquo;s [21]</span><span class="c4">&nbsp;model is superior to ours in terms of sensitivity, but has a lower specificity. </span></p><p class="c17"><span class="c12">Conclusion</span></p><p class="c9"><span class="c5">Our algorithm consisted of a weighted voting of two CNNs (InceptionV3 and Xception) that were trained to classify a patient as having COVID-19, other disease, or</span><span class="c5">&nbsp;being normal</span><span class="c5">.</span><span class="c5">&nbsp;This model achieved an AUC of 0.99, with 93% recall and 97% precision on the COVID-19 class. </span><span class="c5">In contexts where testing with RT-PCR is unfeasible, algorithms such as the one developed in this paper could help screen patients. However, any algorithm must still prove its reliability under proper clinical trials. Nonetheless, this study demonstrates that neural networks may be </span><span class="c5">a valuable tool in fighting COVID-19.</span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c2"><span class="c12"></span></p><p class="c17"><span class="c12">References</span></p><p class="c7"><span class="c4">[1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B. Alberts, A. Johnson, J. Lewis, M. Raff, K. Roberts, and P. Walter, &quot;Introduction to pathogens,&quot; in Molecular Biology of the Cell. 4th edition: Garland Science, 2002.</span></p><p class="c7"><span class="c4">[2]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D. E. Bloom and D. Cadarette, &quot;Infectious Disease Threats in the 21st Century: Strengthening the Global Response,&quot; Frontiers in immunology, vol. 10, p. 549, 2019.</span></p><p class="c7"><span class="c4">[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A. T. Pavia, &quot;Viral infections of the lower respiratory tract: old viruses, new viruses, and the role of diagnosis,&quot; Clinical Infectious Diseases, vol. 52, no. suppl_4, pp. S284-S289, 2011.</span></p><p class="c7"><span class="c4">[4]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Y. Huang, &quot;The SARS epidemic and its aftermath in China: a political perspective,&quot; Learning from SARS: Preparing for the next disease outbreak, pp. 116-36, 2004.</span></p><p class="c7"><span class="c4">[5]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C. f. D. C. a. Prevention. &quot;Middle East Respiratory Syndrome.&quot; https://www.cdc.gov/coronavirus/mers/index.html (accessed.</span></p><p class="c7"><span class="c4">[6]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;W. H. Organization, &quot;Coronavirus disease (COVID-19) Pandemic,&quot; 2020. [Online]. Available: https://www.who.int/emergencies/diseases/novel-coronavirus-2019.</span></p><p class="c7"><span class="c4">[7]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C. f. D. C. a. Prevention, &quot;Coronavirus Disease 2019 (COVID-19) Information for Laboratories,&quot; 2020. [Online]. Available: https://www.cdc.gov/coronavirus/2019-ncov/lab/index.html.</span></p><p class="c7"><span class="c4">[8]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C. f. D. C. a. Prevention, &quot;CDC - 2019-nCoV Real-Time RT-PCR Diagnostic Panel,&quot; 2020. [Online]. Available: https://www.cdc.gov/coronavirus/2019-ncov/downloads/Factsheet-for-Patients-2019-nCoV.pdf.</span></p><p class="c7"><span class="c4">[9]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J. Liu, H. Yu, and S. Zhang, &quot;The indispensable role of chest CT in the detection of coronavirus disease 2019 (COVID-19),&quot; European Journal of Nuclear Medicine and Molecular Imaging, p. 1, 2020.</span></p><p class="c7"><span class="c4">[10]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Y. Li and L. Xia, &quot;Coronavirus Disease 2019 (COVID-19): Role of chest CT in diagnosis and management,&quot; American Journal of Roentgenology, pp. 1-7, 2020.</span></p><p class="c7"><span class="c4">[11]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;T. Ai et al., &quot;Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases,&quot; Radiology, p. 200642, 2020.</span></p><p class="c7"><span class="c4">[12]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;T. W. Post. (2020) How U.S. coronavirus testing stalled: Flawed tests, red tape and resistance to using the millions of tests produced by the WHO</span></p><p class="c7"><span class="c4">. Available: https://www.washingtonpost.com/business/2020/03/16/cdc-who-coronavirus-tests/</span></p><p class="c7"><span class="c4">[13]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E. J. Emanuel et al., &quot;Fair allocation of scarce medical resources in the time of COVID-19,&quot; ed: Mass Medical Soc, 2020.</span></p><p class="c7"><span class="c4">[14]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X. Xuanyang, G. Yuchang, W. Shouhong, and L. Xi, &quot;Computer aided detection of SARS based on radiographs data mining,&quot; in 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference, 2006: IEEE, pp. 7459-7462. </span></p><p class="c7"><span class="c4">[15]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P. Rajpurkar et al., &quot;Chexnet: Radiologist-level pneumonia detection on chest X-rays with deep learning,&quot; arXiv preprint arXiv:1711.05225, 2017.</span></p><p class="c7"><span class="c4">[16]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I. Sirazitdinov, M. Kholiavchenko, T. Mustafaev, Y. Yixuan, R. Kuleev, and B. Ibragimov, &quot;Deep neural network ensemble for pneumonia localization from a large-scale chest X-ray database,&quot; Computers &amp; Electrical Engineering, vol. 78, pp. 388-399, 2019.</span></p><p class="c7"><span class="c4">[17]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A. Narin, C. Kaya, and Z. Pamuk, &quot;Automatic detection of coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks,&quot; arXiv preprint arXiv:2003.10849, 2020.</span></p><p class="c7"><span class="c4">[18]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F. M. Salman, S. S. Abu-Naser, E. Alajrami, B. S. Abu-Nasser, and B. A. Alashqar, &quot;COVID-19 Detection using Artificial Intelligence,&quot; 2020.</span></p><p class="c7"><span class="c4">[19]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J. Zhang, Y. Xie, Y. Li, C. Shen, and Y. Xia, &quot;COVID-19 Screening on Chest X-ray Images Using Deep Learning based Anomaly Detection,&quot; arXiv preprint arXiv:2003.12338, 2020.</span></p><p class="c7"><span class="c4">[20]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F. Shi et al., &quot;Large-Scale Screening of COVID-19 from Community Acquired Pneumonia using Infection Size-Aware Classification,&quot; arXiv preprint arXiv:2003.09860, 2020.</span></p><p class="c7"><span class="c4">[21]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I. D. Apostolopoulos and T. A. Mpesiana, &quot;COVID-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks,&quot; Physical and Engineering Sciences in Medicine, p. 1, 2020.</span></p><p class="c7"><span class="c4">[22] &nbsp; &nbsp;Chowdhury, M.E., et al., Can AI help in screening Viral and COVID-19 pneumonia? arXiv preprint arXiv:2003.13145, 2020.</span></p><p class="c7"><span class="c4">[23] &nbsp; &nbsp;M.E.H. Chowdhury, T.R., A. Khandakar, R. Mazhar, M.A. Kadir, Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I. Reaz, COVID-19 RADIOGRAPHY DATABASE. 2020.</span></p><p class="c7"><span class="c4">[24] &nbsp; Wang, X., et al. Hospital-scale Chest x-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. in IEEE CVPR. 2017.</span></p><p class="c7"><span class="c4">[25] &nbsp; Mahdianpari, Masoud &amp; Salehi, Bahram &amp; Rezaee, Mohammad &amp; Mohammadimanesh, Fariba &amp; Zhang, Yun. (2018). Very Deep Convolutional Neural Networks for Complex Land Cover Mapping Using Multispectral Remote Sensing Imagery. Remote Sensing. 10. 1119. 10.3390/rs10071119. </span></p><p class="c7"><span class="c4">[26] &nbsp;Ai, T., et al., Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases. Radiology, 2020: p. 200642.</span></p><p class="c7 c11"><span class="c4"></span></p><p class="c2"><span class="c4"></span></p><p class="c7 c11"><span class="c4"></span></p></body></html>